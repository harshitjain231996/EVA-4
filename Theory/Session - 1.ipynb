{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color is going to be captured by neural networks, but color is not going to be the main component of the feature which is going to be extracted by deep neural networks.\n",
    "\n",
    "We humans have only 1 sensor for color as compared to 100 sensor for grey, black and white. This is the reason why we can see slightly better at night as well as compared to other animals\n",
    "\n",
    "On camera, we have R, G, and B sensors.\n",
    "\n",
    "In case of imaging sensor, we have 3 color sensors namely R,G and B.\n",
    "\n",
    "In case of human eyes, we have 3 color sensors, R,G,B. In real, we don't have green sensor, we instead have a yellow sensor. Even though we have R,Y,B sensor, it does not change anything for us.\n",
    "\n",
    "THe moment we convert anything into digital form, we get R,G and B colors.\n",
    "\n",
    "Newspapers have 4 colors - C,M,Y,K.\n",
    "\n",
    "We can take our information and divide it into as much details as possible. A painter can use multiple colors to paint and therefore the painter is using multiple channels. When the same painting is converted into a digital format, it is only made by using 3 channels i.e. R,G,B\n",
    "\n",
    "1x1 convolution is going to take any set of channels and it's going to create a new set of channels which are non-destructive and not lossy.\n",
    "\n",
    "We can think of kernel as a filter or a feature extractor.\n",
    "\n",
    "When a featuren extractor or a filter or a kernel is going to be applied on top of an image,it is going to create a channel corresponding to that particular feature.\n",
    "\n",
    "A channel is a container of a same or similar context. We can't call anything a channel unless it contains something of the same context.\n",
    "\n",
    "Simple channels can be combined to make a complicated channel.\n",
    "\n",
    "Edges + gradients ----> Textures\n",
    "\n",
    "Groups of Textures combined = Patterns\n",
    "\n",
    "Groups of Patterns combined = Part of Objects\n",
    "\n",
    "Groups of Part of Objects combined = Objects\n",
    "\n",
    "Combination of Objects = Scenes\n",
    "\n",
    "Whenever our brain looks at a particular object, at that moment, the particular object get's printed on the brain itself. The image of the object get's printed at the back of the head.\n",
    "\n",
    "In our brain, if a particular neuron get's fired for a particular edge, then that particular neuron never get's fired for some other edge.\n",
    "\n",
    "The image which is getting printed on the brain is getting printed on those neurons which are edge detectors and this is the starting of CNN.\n",
    "\n",
    "Neural Networks does extract edges, gradients, textures and patterns from the images. If these things are not extracted, we cannot make complicated stuffs.\n",
    "\n",
    "Deep neural networks are used for extracting most important features from the image and it will train a specific kernel for it.\n",
    "\n",
    "Downsampling -> Reducing the size of the image.\n",
    "\n",
    "Downsampling is required to be done by DNN in order to make sure that the processing done is less. \n",
    "\n",
    "Sometimes we don't need so much of information and this is the reason why we downsample the image so that we can only take the most essential and useful features only. If we can achieve same conclusion with less number of features, then there is no need of so many features.\n",
    "\n",
    "Convolution -> Kernel is going to move on the image step-by-step covering every single pixel possible. This process is called convoltion. Every time kernel is stopping at a location, it has it's own parameters, it is doing computation on those pixels which it is seeing and passing the output to the next layer/image.\n",
    "\n",
    "Whenever we add a new layer, we are increasing the receptive field by 2.\n",
    "\n",
    "Whenever we convolve a 3x3 kernel on 3x3 image,we get 1x1 image.\n",
    "\n",
    "Whenever we convolve a 3x3 kernel on 5x5 image,we get 3x3 image.\n",
    "\n",
    "Whenever we convolve a 3x3 kernel on 7x7 image,we get 5x5 image.\n",
    "\n",
    "Whenever we convolve a 3x3 kernel on 9x9 image,we get 7x7 image.\n",
    "\n",
    "Global Receptive Field -> Each pixel has seen how many pixels is called global receptive field.\n",
    "\n",
    "If we are going to make a network, the last layer of the network should have seen all pixel values. Size of the object = size of the image.\n",
    "\n",
    "They are not complete. Will complete it soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
