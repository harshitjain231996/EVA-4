{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for Session 1- Every point is super important. Do not skip anything!!!\n",
    "\n",
    "Color is going to be captured by neural networks, but color is not going to be the main component of the features which are going to be extracted by deep neural networks since an object can have several colors. \n",
    "\n",
    "We humans have only 1 sensor for color as compared to 100 sensor for grey, black and white. This is the reason why we can see slightly better at night as well as compared to other animals. Our brain fills the color for us. Similar is with Deep Neural Networks. DNN are not going to learn colors. \n",
    "\n",
    "On camera, we have R, G, and B sensors.\n",
    "\n",
    "In case of imaging sensor, we have 3 color sensors namely R,G and B.\n",
    "\n",
    "In case of human eyes, we have 3 color sensors, R,G,B. In real, we don't have green sensor, we instead have a yellow sensor. Even though we have R,Y,B sensor, it does not change anything for us.\n",
    "\n",
    "The moment we convert anything into digital form, we get R,G and B colors.\n",
    "\n",
    "Newspapers have 4 colors - C,M,Y,K. 4 colors of a newspaper can be combined to make a same image which can be made by combining 3 other colors as well. What we can conclude is that we can take information and divide it into as much details as possible. We don't only have R,G,B channels with us. \n",
    "\n",
    "We can take our information and divide it into as much details as possible. A painter can use multiple colors to paint and therefore the painter is using multiple channels. When the same painting is converted into a digital format, it is only made by using 3 channels i.e. R,G,B. \n",
    "\n",
    "Example - Imagine a song is being played and in that song we are listening to a guitarist, a drummer, a pianist playing their instruments and a singer singing as well. So we can say that now we have 4 channels being thrown at us at once. Now imagine just listening to drums being played and nothing else. That's a channel for us. Let's mute everyone and just listen to the guitar being played, that's another channel. Let's mute everyone and just listen to the audio which singer is singing, that's one more channel. When we combine all those 4 channels back, we can create the song back again. That's how exactly we record as well. After recording we would like to make sure that we can actually increase the volume of the drums or the singer maybe becoz the drums are too loud for the music room. \n",
    "\n",
    "1x1 convolution is going to take any set of channels and it's going to create a new set of channels which are non-destructive and not lossy.\n",
    "\n",
    "We can think of kernel as a filter or a feature extractor. \n",
    "\n",
    "When a feature extractor or a filter or a kernel is going to be applied on top of an image,it is going to create a channel corresponding to that particular feature.\n",
    "\n",
    "A channel is a container of a same or similar context. We can't call anything a channel unless it contains something of the same context.\n",
    "\n",
    "Simple channels can be combined to make a complicated channel. Edges and gradients in images are going to be equivalent to alphabets in words or sentences. \n",
    "\n",
    "Edges + gradients ----> Textures\n",
    "\n",
    "Groups of Textures combined = Patterns\n",
    "\n",
    "Groups of Patterns combined = Part of Objects\n",
    "\n",
    "Groups of Part of Objects combined = Objects\n",
    "\n",
    "Combination of Objects = Scenes\n",
    "\n",
    "![s1](https://i.imgur.com/5OAZLfb.jpg)\n",
    "\n",
    "Whenever our brain looks at a particular object, at that moment, the particular object get's printed on the brain itself. The image of the object get's printed at the back of the head. \n",
    "\n",
    "In our brain, if a particular neuron get's fired for a particular edge, then that particular neuron never get's fired for any other edge. \n",
    "\n",
    "The image which is getting printed on the brain is getting printed on those neurons which are edge detectors and this is the starting of CNN and CV. This is the break which scientists needed to understand really how do we work with images. When an image is getting printed, it get's printed on a set of neurons which are edge extractors. \n",
    "\n",
    "Neural Networks does extract edges, gradients, textures and patterns from the images. If these things are not extracted, we cannot make complicated stuffs.\n",
    "\n",
    "Deep neural networks are used for extracting most important features from the image and also it will train a specific kernel for it. \n",
    "\n",
    "Downsampling -> Reducing the size of the image.\n",
    "\n",
    "Downsampling is required to be done by DNN in order to make sure that the processing done is less. \n",
    "\n",
    "Sometimes we don't need so much of information and this is the reason why we downsample the image so that we can only take the most essential and useful features only. If we can achieve same conclusion with less number of features, then there is no need of so many features.\n",
    "\n",
    "A kernel is given all the channels and the kernel is going to decide which channels it is going to work with. \n",
    "\n",
    "Convolution -> Kernel is going to move on the image step-by-step covering every single pixel possible. This process is called convoltion. Every time kernel is stopping at a location, it has it's own parameters, it is doing computation on those pixels which it is seeing and passing the output to the next layer/image.\n",
    "\n",
    "Generally we have pixels in the range of 0-255. But when we work with Deep neural networks, 255 numbers are not good enough for us. We need a large set of numbers. So for deep neural networks we use floating point numbers to work with. We convert 255 to 1 and we take values from 0 to 1. Between 0 and 1, we can have infinitely floating point numbers. This helps us to expand our range and helps the deep neural networks to work with nicely. \n",
    "\n",
    "Our kernels are also defined between -1 to +1 during initialization part. Maybe after training, they can get a number which is beyond this range also but mostly -1 to +1 is the range.\n",
    "\n",
    "A stride of 1 means that the kernel is moving only 1 pixel at a time on the image. \n",
    "\n",
    "A deep neural network should have seen the whole image to give us results and this is what's called a receptive field.\n",
    "\n",
    "Local receptive field - The number of pixels a kernel can see is called it's local receptive field.\n",
    "\n",
    "The reason we use a 3x3 kernel is becoz we can make any kernel out of it we want. Example - When we do two 3x3 i.e. one after the other we are actually doing a 5x5. If we want a 7x7 kernel, we will use a 3x3 thrice. \n",
    "\n",
    "Whenever we add a new layer, we are increasing the receptive field by 2.\n",
    "\n",
    "Whenever we convolve a 3x3 kernel on 3x3 image,we get 1x1 image.\n",
    "\n",
    "Whenever we convolve a 3x3 kernel on 5x5 image,we get 3x3 image.\n",
    "\n",
    "Whenever we convolve a 3x3 kernel on 7x7 image,we get 5x5 image.\n",
    "\n",
    "Whenever we convolve a 3x3 kernel on 9x9 image,we get 7x7 image.\n",
    "\n",
    "![con1](https://i.imgur.com/24Hvfm1.jpg)\n",
    "\n",
    "![con2](https://i.imgur.com/Au5xq0F.jpg)\n",
    "\n",
    "Global Receptive Field -> Each pixel has seen how many pixels is called global receptive field.\n",
    "\n",
    "![r1](https://i.imgur.com/vuRJpys.jpg)\n",
    "\n",
    "If we are going to make a network of 400x400, the last layer of the network should have seen all the 400x400 pixels i.e. the whole image.\n",
    "\n",
    "If we are going to make a network, the last layer of the network should have seen all pixel values.\n",
    "\n",
    "Example - If we convolve a 5x5 on 5x5, we get a 1x1. If the output is 5x5 and we have used a 5x5 kernel, the input size should be 9x9 i.e. an increase in receptive field by 4. If we used a 3x3 on a 400x400 image, we had to add 200 layers. However, if we used 5x5 the total numbers of layers required would be 100. If we used 11x11 kernel then we would have needed 40 layers. Then why are we going with 3x3? Reason - When we convolve a 3x3 on 5x5 we get 3x3 and again when we convolve a 3x3 on the resultant 3x3, we get 1x1. If we had used a 5x5 on 5x5, we would have directly got 1x1. So in the first case, a total of 3x3+3x3 = 9+9 = 18 parameters. In the second case, 5x5 = 25 parameters. When we use 3x3 to get to the same receptive field, we need far less number of parameters. A 7x7 would give me a total of 49 parameters whereas if we would have used 3x3 three times we would have got the total number of parameters as 27.\n",
    "\n",
    "![1](https://i.imgur.com/RbzjYT9.jpg)\n",
    "\n",
    "When we use a kernel of size 3x3, the number of parameters required are significantly less and this ultimately also leads to a decrease in the number of multiplications required. In most of the cases it's true if maybe in not all the cases. Researchers found out that when we use a 3x3 kernel, the RAM required is less and computation hence is faster. In most of the cases, 3x3 kernel is going to be the only kernel we are going to need. \n",
    "\n",
    "A 3x3 kernel has a central line which is missing in the case of 2x2 kernel. Having a central line means that it can create symmetric objects. \n",
    "\n",
    "How many times to we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)??\n",
    "\n",
    "Answer - 199x199 convolved on 3x3 we get 197x197, then 197x197 convolved on 3x3 gives us 195x195, again 195x195 convolves on 3x3 to give us 193x193. Eventually, in order to reach to 1x1, we would need a total of 100 layers. A network of 100 layers is really huge. \n",
    "\n",
    "When we work with a network of 1000 layers, we need a lot of money and these many layers is used for those highly critical applications such as defence where we need extreme accurate precision such as to differentiate between whether it is a fighter jet or commerical jet. If we need to solve simple tasks where we do not need very high precision, we can work with a network which has very few layers such as maybe 15,20 or even 30 layers. Number of layers is linked to the hardware we are going to run on, the amount of data set we have i.e. if we have a big model or if we have deep layers we need to have large dataset also. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
